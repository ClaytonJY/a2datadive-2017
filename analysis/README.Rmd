---
title: "Huron River Watershed Council - A2 Data Dive 2017"
author: "Clayton Yochum"
date: "November 12, 2017"
output:
  html_document:
    code_folding: hide
    toc: yes
---

```{r setup, include=FALSE}
library(dplyr)
library(purrr)
library(readr)
library(tidyr)
library(ggplot2)
library(RcppRoll)
library(lubridate)
library(forcats)

library(knitr)

theme_set(theme_bw())

opts_chunk$set(echo = TRUE, fig.width = 8.5, fig.height = 8.5)
```

This document is an analysis of data provided by the Huron River Watershed Council as part of the 2017 A2 Data Dive.

Click the "Code" buttons above tables and plots to see the R code that generated them.


## Data Preparation

### Dam River Flow

We'll start with pre-processed data measuring water flow below three dams on the Huron River. In order from north to south they are New Hudson, Hamburg, and Wall Street.

New Hudson has only daily readings and stops in 2014, but has no missing data. The other two have measurements in 15-minute intervals and have had some missing data imputed, while other gaps remain.

```{r dam-tables}

dam_tbls <- list(
  `New Hudson`  = "../Huron River Watershed Council/Data/new_hudson_preprocessed.csv",
  `Hamburg`     = "../Huron River Watershed Council/Data/hamburg_preprocessed.csv",
  `Wall Street` = "../Huron River Watershed Council/Data/wall_street_preprocessed.csv"
) %>%
  map(
    read_csv,
    skip = 1,
    col_names = c("datetime", "flow"),
    col_types = "Td"
  ) %>%
  map(arrange, datetime)

dam_tbl <- dam_tbls %>%
  bind_rows(.id = "site") %>%
  mutate(site = factor(site, levels = names(dam_tbls)))

dam_tbl %>%
  group_by(site) %>%
  slice(c(1:3, (n() - 2):n())) %>%
  kable()
```

We can make sure the time intervals and endpoints match what we expect.

```{r summarize-dam-tbls}

dam_tbl %>%
  group_by(site) %>%
  summarize(
    start    = min(datetime),
    stop     = max(datetime),
    n        = n(),
    avg_flow = mean(flow)
  ) %>%
  kable()
```

Can we plot it?

```{r plot-flows}

dam_tbl %>% 
  ggplot(aes(x = datetime, y = flow, color = site)) +
  facet_wrap(~site, ncol = 1) +
  geom_line()
```

There's a pretty big difference in magnitude of flow between the sites, which is expected as the cross-sections of the river are different sizes at each. We can also free the Y axis to see more of the within-site variance of flow.

```{r plot-flows-enhanced}

dam_tbl %>%
  ggplot(aes(x = datetime, y = flow, color = site)) +
  geom_line() +
  facet_wrap(~site, ncol = 1, scales = "free_y")
```

This obscures any year-to-year similarity. Let's superimpose years atop one another

```{r plot-flows-yearly}

set_year <- function(x, y) {
  year(x) <- y
  x
}

dam_tbl %>%
  mutate(
    year         = year(datetime),
    new_datetime = set_year(datetime, max(year))
  ) %>%
  select(site, new_datetime, year, flow) %>%
  filter(complete.cases(.)) %>% 
  ggplot(aes(x = new_datetime, y = flow, color = site)) +
  facet_wrap(~site, ncol = 1, scales = "free_y") +
  geom_line(aes(group = year, alpha = year)) +
  geom_line(data = . %>% group_by(site, new_datetime) %>% summarize(flow = mean(flow, na.rm = TRUE)), size = 1.5) +
  scale_x_datetime(date_breaks = "1 month", date_labels = "%b") +
  labs(
    x     = "Day of Year",
    y     = bquote('Flow ('*ft^3 / s*')'),
    alpha = "Year",
    title = "Flow by Site and Day-of-Year",
    subtitle = "with average line in bold"
  ) +
  guides(color = FALSE)
```

We can see the Wall Street data is much less smooth day-to-day than readings at the other sites. This could be due to

- the flow gauge being much closer to the damn upstream than gauges at the other sites are
- instrumentation or measurement error
- issues in the imputatation already done on this data-
- operational differences (this is the only automated damn)

Or some mix of multiple causes.


#### Missing Data

While some missing values have been imputed for Hamburg and Wall Street, we'll make remaining missing values explicit.

```{r fill-missing-flows}

dam_tbls <- dam_tbls %>%
  modify_at(
    c("Hamburg", "Wall Street"),
    ~ .x %>%
      complete(
        datetime = seq(min(.$datetime), max(.$datetime), by = as.difftime(15, units = "mins")),
        fill     = list(flow = NA_real_)
      )
  )

dam_tbl <- dam_tbls %>%
  bind_rows(.id = "site") %>%
  mutate(site = factor(site, levels = names(dam_tbls)))

dam_tbl %>%
  group_by(site) %>%
  summarize(
    start       = min(datetime),
    stop        = max(datetime),
    n           = n(),
    n_missing   = sum(is.na(flow)),
    pct_missing = n_missing / n,
    avg_flow    = mean(flow, na.rm = TRUE)
  ) %>%
  kable()
```

We see a _lot_ of missing data in Hamburg, over 10%!


### Rainfall

There's also a set of rain gauge data from the Barton Pond, which is near the Wall Street gauge.

Rainfall is measured daily, in inches.

```{r read-rainfall}

rain_tbl <- read_csv(
  "../Huron River Watershed Council/Data/barton_pond_raingauge_data.csv",
  skip = 1,
  col_names = c("date", "rainfall"),
  col_types = "Dd"
)

rain_tbl %>%
  head() %>%
  kable()
```

And a summary again

```{r summarize-rainfall}

rain_tbl %>%
  summarize(
    start     = min(date),
    stop      = max(date),
    n         = n(),
    n_missing = sum(is.na(rainfall)),
    avg_rain  = mean(rainfall)
  ) %>%
  kable()
```

And a plot again

```{r plot-rainfall}

rain_tbl %>%
  ggplot(aes(x = date, y = rainfall)) +
  geom_line() +
  geom_smooth(method = "gam")
```

Here it is yearly:

```{r plot rainfall yearly}

rain_tbl %>%
  mutate(
    year     = year(date),
    new_date = set_year(date, max(year))
  ) %>%
  filter(!is.na(new_date)) %>%
  ggplot(aes(x = new_date, y = rainfall)) +
  geom_line(aes(group = year, alpha = year)) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  labs(
    x = "Day of Year",
    y = "Rainfall (in.)",
    alpha = "Year",
    title = "Year-over-Year Daily Rainfall",
    subtitle = "at Barton Pond"
  )
```

Very noisy day-to-day; how about monthly totals?

```{r plot-rainfall-monthly-sum}

rain_tbl %>%
  group_by(year = year(date), new_date = floor_date(date, unit = "month") %>% set_year(max(year))) %>%
  summarize(rainfall = sum(rainfall)) %>%
  ggplot(aes(x = new_date, y = rainfall)) +
  geom_line(aes(group = year, alpha = year)) +
  geom_line(data = . %>% group_by(new_date) %>% summarize(rainfall = mean(rainfall)), size = 1.5) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  labs(
    x = "Month",
    y = "Rainfall (in.)",
    alpha = "Year",
    title = "Monthly Rainfall Totals",
    subtitle = "at Barton Pond"
  )
```

We see an unsurprising trend of higher rainfall in the spring, with less than half as much during winter.


## Main Questions

### Sudden Fluctuation in Flow

From the instructions (`General Instructions.pdf`):

> Sudden fluctuations are changes in flow by 150% or above within a 12 hour period.

Since New Hudson is measured in daily intervals, we'll do this with the other two first.

```{r average-flow}

# 12 hours prior should be 12*4=48 points prior due to 15-minute interval
avg_flow_tbl <- dam_tbl %>%
  filter(site != "New Hudson") %>%
  group_by(site) %>% 
  arrange(datetime) %>%
  mutate(
    lag = lag(flow, 1L),
    past_avg_flow = roll_meanr(lag,       48, na.rm = TRUE),
    past_missing  = roll_sumr(is.na(lag), 48) 
  ) %>%
  ungroup() %>%
  select(-lag) %>%
  filter(!is.na(past_missing)) %>%                              # remove early entires without history
  mutate(pct_change = (flow - past_avg_flow) / past_avg_flow)

avg_flow_tbl %>%
  group_by(site) %>%
  slice(c(1:3, (n() - 2):n())) %>%
  kable()
```

How many of our samples are more than 150% or 100% higher than the past 12-hour average?

```{r plot-average-flow}

avg_flow_tbl %>%
  group_by(site) %>%
  summarize(
    n_over_150   = sum(pct_change >= 1.5, na.rm = TRUE),
    pct_over_150 = n_over_150 / n(),
    n_over_100   = sum(pct_change >= 1.0, na.rm = TRUE),
    pct_over_100 = n_over_100 / n()
  ) %>%
  kable()
```

So we see there are no cases at the Hamburg site where flow was above 200% of the trailing 12-hour average, but there are a handful of such cases at the Wall Street site.


#### New Hudson

We can't do quite the same calculation for New Hudson since it's flow is only available per-day, but we can see which points are above the thresholds when compared to the prior days' flow. This is a lot simpler to compute, and we also don't have to sweat missing data.

```{r new-hudson-average-flow}

new_hudson_tbl <- dam_tbl %>%
  filter(site == "New Hudson") %>%
  arrange(datetime) %>%
  mutate(
    past_avg_flow = lag(flow, 1L),
    pct_change = (flow - past_avg_flow) / past_avg_flow
  ) %>%
  filter(!is.na(past_avg_flow))

new_hudson_tbl %>%
  summarize(
    n_over_150   = sum(pct_change >= 1.5),
    pct_over_150 = n_over_150 / n(),
    n_over_100   = sum(pct_change >= 1.0),
    pct_over_100 = n_over_100 / n()
  ) %>%
  kable()
```

So we see just a few days have a flow over the the thresholds compared to the previous day; might as well pull them all.

```{r new-hudson-high-flows}

new_hudson_tbl %>%
  filter(pct_change > 1.0) %>%
  arrange(desc(pct_change)) %>%
  kable()
```

Plotting them might help show if this is a solved problem, or if these threshold-breaking increases still happen.

```{r plot-average-flows}

avg_flow_tbl <- bind_rows(new_hudson_tbl, avg_flow_tbl)

avg_flow_tbl %>%
  filter(!is.na(pct_change)) %>%
  ggplot(aes(x = datetime, y = pct_change, color = site)) +
  facet_wrap(~site, ncol = 1, scales = "free_y") +
  geom_line() +
  geom_hline(yintercept = c(1, 1.5), linetype = 2) +
  labs(
    x = "Date",
    y = "Change in Flow",
    title = "Change in River Flow Over Time"
  ) +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::percent)
```


### Flow Targets

The USGS sets targets for a minimum level of flow beyond each dam. How often do we dip below those?

```{r read-flow-targets}

# manually extracted from "Example Numeric Flow Targets.docx"
flow_targets <- read_csv("../Huron River Watershed Council/Data/flow_targets.csv", col_types = "ciiii") %>%
  mutate(site = factor(site, levels = unique(site), labels = levels(dam_tbl$site)))

kable(flow_targets)
```

We can count how often we're below the minimums

```{r count-low-flows}

dam_tbl %>%
  left_join(flow_targets, by = "site") %>%
  filter(!is.na(flow)) %>%
  group_by(site) %>%
  summarize(
    n            = n(), 
    n_low_flow   = sum(flow < min_flow),
    pct_low_flow = n_low_flow / n 
  ) %>%
  kable()
```

So we see that most dams tend to experience some time below the threshold. The Wall Street site in particular spends a lot of time underneath the prescribed minimum flow.

Let's plot our flows again, with the minimum flow limits superimposed.

```{r plot-low-flows}

temp <- dam_tbl %>%
  mutate(
    year         = year(datetime),
    new_datetime = set_year(datetime, max(year))
  ) %>%
  filter(complete.cases(.))

targets_long <- flow_targets %>%
  gather(limit, flow, -site)

limits <- targets_long %>%
  filter(limit %in% c("min_flow")) %>% 
  mutate(new_datetime = min(temp$new_datetime))

temp %>%
  ggplot(aes(x = new_datetime, y = flow, color = site)) +
  facet_wrap(~site, ncol = 1, scales = "free_y") +
  geom_line(aes(group = year, alpha = year)) +
  geom_line(data = . %>% group_by(site, new_datetime) %>% summarize(flow = mean(flow, na.rm = TRUE)), size = 1.5) +
  geom_hline(data = limits, aes(yintercept = flow), linetype = 2) +
  geom_text(
    data = limits,
    aes(x = new_datetime, y = flow, label = flow),
    vjust = -1, hjust = 1.1, color = "black", size = 3
  ) +
  scale_x_datetime(date_breaks = "1 month", date_labels = "%b") +
  labs(
    x     = "Day of Year",
    y     = bquote('Flow ('*ft^3 / s*')'),
    alpha = "Year",
    title = "Flow by Site And Year",
    subtitle = "with minimum flow guidelines (dotted line)"
  ) +
  guides(color = FALSE)
```


#### Spring Storm Season

We can also focus on the spring storm season (April 15 - June 30). What percent of the time are we outside the prescribed limits?

```{r spring-outside-limits}

spring_only <- temp %>%
  filter(
    new_datetime >= as.POSIXct(paste0(unique(year(new_datetime)), "-04-15"), tz = "UTC"),
    new_datetime <  as.POSIXct(paste0(unique(year(new_datetime)), "-07-01"), tz = "UTC")
  ) %>%
  left_join(flow_targets, by = "site")

spring_only %>%
  select(site, flow, spring_low_flow, spring_high_flow) %>%
  filter(!is.na(flow)) %>%
  group_by(site) %>%
  summarize(
    n                = n(), 
    n_outside_limits = sum(pmap_dbl(list(flow, spring_low_flow, spring_high_flow), ~ !between(..1, ..2, ..3))),
    pct_outside      = n_outside_limits / n 
  ) %>%
  kable()
```

Again, plenty of operation outside the prescribed limits. We can see this visually by plotting flow once again and imposing the spring-specific guideline on top.

```{r plot-spring-flows}

limits <- targets_long %>%
  filter(limit %in% grep("spring", limit, value = TRUE)) %>% 
  mutate(new_datetime = min(spring_only$new_datetime))

spring_only %>%
  ggplot(aes(x = new_datetime, y = flow, color = site)) +
  facet_wrap(~site, ncol = 1, scales = "free_y") +
  geom_line(aes(group = year, alpha = year)) +
  geom_line(data = . %>% group_by(site, new_datetime) %>% summarize(flow = mean(flow, na.rm = TRUE)), size = 1.5) +
  geom_hline(
    data = limits,
    aes(yintercept = flow), 
    linetype = 2
  ) +
  geom_text(
    data = limits,
    aes(x = new_datetime, y = flow, label = flow),
    vjust = -1, hjust = 1.1, color = "black", size = 3
  ) +
  scale_x_datetime(date_breaks = "1 month", date_labels = "%B") +
  labs(
    x     = "Date",
    y     = bquote('Flow ('*ft^3 / s*')'),
    alpha = "Year",
    title = "Spring Flow by Site and Year",
    subtitle = "with flow guidelines (dotted lines)"
  ) +
  guides(color = FALSE)
```


## What to do next

- Use the rainfall data to distinguish rain-driven spikes in flow from mand-made spikes due to dam operation
- Build a predictive model for future flow using historic flow data and rainfall

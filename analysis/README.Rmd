---
title: "Huron River Watershed Council - A2 Data Dive 2017"
author: "Clayton Yochum"
date: "November 11, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(dplyr)
library(purrr)
library(readr)
library(ggplot2)
library(RcppRoll)
library(lubridate)
library(forcats)

library(knitr)

opts_chunk$set(echo = TRUE)
```

## Data Sanity Checks

## Dam River Flow

We'll start with the pre-processed data measuring water flow below three dams on the Huron River. In order from north to south they are New Hudson, Hamburg, and Wall Street.

New Hudson has only daily readings, stops in 2014, but has no missing data. The other two have measurements in 15-minute intervals and have had some missing data imputed, while other gaps remain.

```{r}

dam_tbl <- list(
  new_hudson  = "../Huron River Watershed Council/Data/new_hudson_preprocessed.csv",
  hamburg     = "../Huron River Watershed Council/Data/hamburg_preprocessed.csv",
  wall_street = "../Huron River Watershed Council/Data/wall_street_preprocessed.csv"
) %>%
  map_dfr(
    read_csv,
    skip = 1,
    col_names = c("datetime", "discharge"),
    col_types = "Td",
    .id = "site"
  ) %>%
  mutate(site = factor(site, levels = unique(site)))

dam_tbl %>%
  head() %>%
  kable()
```

We can make sure the time intervals and endpoints match what we expect.

```{r}

dam_tbl %>%
  group_by(site) %>%
  summarize(
    start         = min(datetime),
    stop          = max(datetime),
    n             = n(),
    avg_discharge = mean(discharge)
  ) %>%
  kable()
```

Can we plot it?

```{r}

daily_tbl <- dam_tbl %>%
  group_by(site, date = as.Date(datetime)) %>%
  summarize(mean_discharge = mean(discharge))

daily_tbl %>% 
  ggplot(aes(x = date, y = mean_discharge, color = site)) +
  geom_line() +
  facet_wrap(~site, ncol = 1)
```

We can also free the Y axis and add a smoothing line to get a sense of the baseline differences between sites.

```{r}

daily_tbl %>%
  ggplot(aes(x = date, y = mean_discharge, color = site)) +
  geom_line() +
  geom_smooth(method = "gam") +
  facet_wrap(~site, ncol = 1, scales = "free_y")
```

### Rainfall

There's also a set of rain gauge data from the Barton Pond.

Rainfall is measured daily, in inches.

```{r}

rain_tbl <- read_csv(
  "../Huron River Watershed Council/Data/barton_pond_raingauge_data.csv",
  skip = 1,
  col_names = c("date", "rainfall"),
  col_types = "Dd"
)

rain_tbl %>%
  head() %>%
  kable()
```

And a summary again

```{r}

rain_tbl %>%
  summarize(
    start    = min(date),
    stop     = max(date),
    n        = n(),
    avg_rain = mean(rainfall)
  ) %>%
  kable()
```

And a plot again

```{r}

rain_tbl %>%
  ggplot(aes(x = date, y = rainfall)) +
  geom_line() +
  geom_smooth(method = "gam")
```


## Main Questions

### Sudden Fluctuation in Flow

From the instructions, 

> Sudden fluctuations are changes in flow by 150% or above within a 12 hour period.

Since New Hudson is measured in daily intervals, we'll do this with the other two first.

```{r}

# 12 hours prior should be 12*4=48 points prior due to 15-minute interval
avg_flow_tbl <- dam_tbl %>%
  filter(site != "new_hudson") %>%
  group_by(site) %>% 
  arrange(datetime) %>%
  mutate(
    lag = lag(datetime, 48),
    past_avg_discharge = roll_meanr(lag(discharge, 1), 48)
  ) %>%
  ungroup() %>%
  filter(
    difftime(datetime, lag, units = "hours") == 12  # only periods with full data
  ) %>%
  select(-lag) %>%
  mutate(pct_change = (discharge - past_avg_discharge) / past_avg_discharge)

avg_flow_tbl %>%
  group_by(site) %>%
  slice(1:3) %>%
  kable()
```

How many of our samples are more than 150% or 100% higher than the past 12-hour average?

```{r}

avg_flow_tbl %>%
  group_by(site) %>%
  summarize(
    n_over_150 = sum(pct_change >= 1.5),
    n_over_100 = sum(pct_change >= 1.0)
  ) %>%
  kable()
```

So we see there are no cases at the Hamburg site where flow was above 200% of the trailing 12-hour average, but there are a handful of such cases at the Wall Street site.

#### New Hudson

We can't do quite the same calculation for New Hudson since it's flow is only available per-day, but we can see which points are above the thresholds when compared to the prior days' flow. This is a lot simpler to compute, and we also don't have to sweat missing data.

```{r}

new_hudson_tbl <- dam_tbl %>%
  filter(site == "new_hudson") %>%
  arrange(datetime) %>%
  mutate(
    past_avg_discharge = lag(discharge, 1L),
    pct_change = (discharge - past_avg_discharge) / past_avg_discharge
  )

new_hudson_tbl %>%
  summarize(
    n_over_150 = sum(pct_change >= 1.5, na.rm = TRUE),
    n_over_100 = sum(pct_change >= 1.0, na.rm = TRUE)
  ) %>%
  kable()
```

So we see just a few days have a flow over the the thresholds compared to the previous day; might as well pull them all.

```{r}

new_hudson_tbl %>%
  filter(pct_change > 1.0) %>%
  arrange(desc(pct_change)) %>%
  kable()
```

Plotting them might help show if this is a solved problem, or if these threshold-breaking increases still happen.

```{r, warning=FALSE}

bind_rows(
  new_hudson_tbl,
  avg_flow_tbl
) %>%
  mutate(site = fct_recode(site, `New Hudson` = "new_hudson", Hamburg = "hamburg", `Wall Street` = "wall_street")) %>%
  group_by(site, date = date(datetime)) %>%
  summarize(max_change = max(pct_change)) %>%
  ggplot(aes(x = date, y = max_change, color = site)) +
  facet_wrap(~site, ncol = 1, scales = "free_y") +
  geom_line() +
  geom_hline(yintercept = c(1, 1.5), linetype = 2) +
  labs(
    x = "Date",
    y = "Change in Flow",
    title = "Change in River Flow Over Time"
  ) +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::percent)
```


### Flow Targets

The USGS sets targets for a minimum level of flow beyond each dam. How often do we dip below those?

```{r}

# manually extracted from "Example Numeric Flow Targets.docx"
flow_targets <- read_csv("../Huron River Watershed Council/Data/flow_targets.csv", col_types = "ciiii") %>%
  mutate(site = factor(site, levels = levels(dam_tbl$site)))

dam_tbl %>%
  group_by(site, date = date(datetime)) %>%
  summarize(lowest_flow = min(discharge)) %>%
  left_join(flow_targets, by = "site") %>%
  ggplot(aes(x = date, y = lowest_flow, color = site)) +
  geom_line() +
  facet_wrap(~site, ncol = 1, scales = "free_y") +
  geom_hline(aes(yintercept = min_flow), linetype = 2)
```

So we see that most dams tend to experience some time below the threshold. The Wall Street site in particular spends a lot of time underneath the prescribed minimum flow.


